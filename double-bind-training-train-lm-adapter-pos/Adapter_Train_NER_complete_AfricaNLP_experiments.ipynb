{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbmKjYRj6OM2"
      },
      "source": [
        "# African NER Notebook\n",
        "\n",
        "This notebook is designed for experiments with language adapters as implemented using the [AdapterHub codebase](adapterhub.ml/). It is designed so that you can run every cell straight from top to bottom, and all the settings are in one place. \n",
        "\n",
        "\n",
        "It can: \n",
        "1. Train a language adapter using some monolingual data, by default a Hausa model trained using data from the MAFAND dataset. \n",
        "2. Fine-tune on a downstream tasks using that adapter to inject linguistic knowledge.  Masakhane NER using that monolingual data.\n",
        "\n",
        "The notebook is divided up into sections: \n",
        "0. Setup: This is where we do necessary installations, downloads, etc. \n",
        "1. Settings (EDIT THIS). This is where you set everything for your experiment. You pick which language to use, etc. \n",
        "2. Training a language adapter\n",
        "3. Using the language adapter to finetune on NER. \n",
        "\n",
        "BEFORE YOU BEGIN: \n",
        "* Make sure you setup your Weights and Biases account first and are part of the Masakhane team (https://wandb.ai/double-bind-ner)! \n",
        "* Add your name and language to this Google sheet. https://docs.google.com/spreadsheets/d/1Its1Yjcr0WHE9vFbuSGuuIYOqDS13qIYXVV_Zk7oCtM/edit#gid=510456181\n",
        "* In Google Colab's \"Runtime\" settings, click \"Change Runtime type\" and be sure you are using a GPU. \n",
        "\n",
        "WHEN YOU ARE READY:\n",
        "* go to the Settings section below\n",
        "* edit your language and other settings you wish to change. \n",
        "* run each cell from top to bottom in the whole notebook. \n",
        "\n",
        "\n",
        "If you have any questions, ask in the slack (this link here: https://masakhane-nlp.slack.com/archives/C04HFLKU17D), or contact Colin Leong (cleong1@udayton.edu) or Herumb Shandilya.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup (do not edit)\n"
      ],
      "metadata": {
        "id": "B-_zdKBIY94O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyMJ-uU8tzj0"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/krypticmouse/double-bind-training.git\n",
        "%cd double-bind-training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgtGux6Lt7PW"
      },
      "outputs": [],
      "source": [
        "!git checkout train-lm-adapter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jTFochFt9ld"
      },
      "outputs": [],
      "source": [
        "! pip install adapter-transformers seqeval ptvsd wandb datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Login to Weights and Biases for experiment tracking\n",
        "Used to save all the metrics and settings for later reference. "
      ],
      "metadata": {
        "id": "E5YyuKYWZkoA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVt-1YPlt_S5"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Settings (Edit this)\n",
        "\n",
        "This is where you set language and other settings. \n",
        "\n",
        "Batch sizes are important because they are the main thing you change based on what GPU you have. If you get \"out of memory\" errors, this is the main one to change. "
      ],
      "metadata": {
        "id": "i1vXuvOtbfsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "language_code = \"hau\" #@param {type:\"string\"}\n",
        "model_name  = \"roberta-base\" #@param {type:\"string\"}\n",
        "import os\n",
        "os.environ[\"language_code\"] = language_code\n",
        "# TODO: add tags in wandb for: \n",
        "# * language code\n",
        "# * model name\n",
        "# * GPU? \n",
        "# etc."
      ],
      "metadata": {
        "id": "1ILCxvZFbrax",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Adapter LM Training Settings \n",
        "per_device_train_batch_size = 8 #@param {type:\"number\"}\n",
        "per_device_eval_batch_size = 8 #@param {type:\"number\"}\n",
        "epochs = 20 #@param {type:\"number\"}\n",
        "\n",
        "%env lm_adapter_training_model_name_or_path model_name\n",
        "%env lm_adapter_training_per_device_train_batch_size per_device_train_batch_size\n",
        "%env lm_adapter_training_per_device_eval_batch_size per_device_eval_batch_size\n",
        "%env lm_adapter_training_num_train_epochs epochs"
      ],
      "metadata": {
        "id": "e5vqArjYYnyP",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title MASAKHANE NER Settings\n",
        "\n",
        "%env MAX_LENGTH=164\n",
        "%env ADAPTER_MODEL=model_name\n",
        "%env OUTPUT_DIR=hau_ner\n",
        "%env BATCH_SIZE=32\n",
        "%env NUM_EPOCHS=20\n",
        "%env SAVE_STEPS=10000\n",
        "%env SEED=1"
      ],
      "metadata": {
        "id": "LXkeSaVTYnqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training a Language Adapter"
      ],
      "metadata": {
        "id": "zaKI8xB2egs8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYx6Vo3p2jUx"
      },
      "source": [
        "### download and preprocess training data from MAFAND\n",
        "https://huggingface.co/datasets/masakhane/mafand is the Mafand dataset. It is intended for machine translation but we can just use it for language modeling. \n",
        "\n",
        "['en-amh', 'en-hau', 'en-ibo', 'en-kin', 'en-lug', 'en-nya', 'en-pcm', 'en-sna', 'en-swa', 'en-tsn', 'en-twi', 'en-xho', 'en-yor', 'en-zul', 'fr-bam', 'fr-bbj', 'fr-ewe', 'fr-fon', 'fr-mos', 'fr-wol'] are the available translation sets. \n",
        "\n",
        "We simply download the first dataset in the list containing the requested language code. \n",
        "\n",
        "TODO: code to deal with other datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xSuRdcKHD-C"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "MAFAND_SETS = ['en-amh', 'en-hau', 'en-ibo', 'en-kin', 'en-lug', 'en-nya', 'en-pcm', 'en-sna', 'en-swa', 'en-tsn', 'en-twi', 'en-xho', 'en-yor', 'en-zul', 'fr-bam', 'fr-bbj', 'fr-ewe', 'fr-fon', 'fr-mos', 'fr-wol'] \n",
        "MAFAND_SETS_containing_code = [config_name for config_name in MAFAND_SETS if language_code in config_name]\n",
        "\n",
        "dataset = load_dataset(\"masakhane/mafand\", MAFAND_SETS_containing_code[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw5CE_8g4sS8"
      },
      "source": [
        "#### Converting translation to language modeling set\n",
        "\n",
        "Our code expects the dataset to be in .txt files, we oblige. \n",
        "\n",
        "Pull out one language, convert to language modeling set. Language modeling sets are monolingual, and each data item has a \"text\" field like this example from https://colab.research.google.com/github/Adapter-Hub/adapter-transformers/blob/master/notebooks/01_Adapter_Training.ipynb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvyVFKI-42pu"
      },
      "outputs": [],
      "source": [
        "flat_dataset = dataset.flatten()\n",
        "flat_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we drop/remove the data that is not in the target language. "
      ],
      "metadata": {
        "id": "NDlgg0M0cO16"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_p7NnTR2x1x"
      },
      "outputs": [],
      "source": [
        "for split in flat_dataset:\n",
        "  for feature in flat_dataset[split].features:\n",
        "    print(feature)\n",
        "    if language_code not in feature:\n",
        "      flat_dataset[split] = flat_dataset[split].remove_columns(feature)\n",
        "  # flat_dataset[split] = flat_dataset[split].remove_columns('translation.en')\n",
        "flat_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have removed the English/French, leaving us with just the target language. Now we write it out to .txt files. "
      ],
      "metadata": {
        "id": "tNbRJwdreUT_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYjVwBYUHWVr"
      },
      "outputs": [],
      "source": [
        "for split in flat_dataset:\n",
        "  split_strings = []\n",
        "  for data_item in flat_dataset[split]:\n",
        "    values = data_item.values()\n",
        "    for value in values: \n",
        "      # print(value)\n",
        "      split_strings.append(value)\n",
        "  with open(f\"{split}.txt\", \"w\") as spf:\n",
        "    spf.writelines('\\n'.join(split_strings))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "use `pwd` command to make sure we're in the right directory"
      ],
      "metadata": {
        "id": "zj8NYZxTay9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "XOiYkkyNayXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional: if restarting from scratch and you want to remove the existing output directory, you can uncomment this and run it to delete the directory."
      ],
      "metadata": {
        "id": "gNaD3Ij7bAlQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sWHINZJYQTX"
      },
      "outputs": [],
      "source": [
        "# !rm -rf /tmp/test-mlm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use `ls` command to see if we've got the training files made correctlyy"
      ],
      "metadata": {
        "id": "GTxFdkgMa4dF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "IE8wqefda_OA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actually run the  Adapter training\n"
      ],
      "metadata": {
        "id": "K7AFmyTlfV4B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvAFw4rFLC51"
      },
      "outputs": [],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0,1 python train_lm_adapter.py \\\n",
        "    --model_name_or_path \"$lm_adapter_training_model_name_or_path\" \\\n",
        "    --train_file train.txt \\\n",
        "    --validation_file validation.txt \\\n",
        "    --per_device_train_batch_size \"$lm_adapter_training_per_device_train_batch_size\" \\\n",
        "    --per_device_eval_batch_size \"$lm_adapter_training_per_device_eval_batch_size\" \\\n",
        "    --train_adapter \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --num_train_epochs \"$lm_adapter_training_num_train_epochs\" \\\n",
        "    --report_to \"wandb\" \\\n",
        "    --run_name \"adapter-training-lm-test\" \\\n",
        "    --output_dir /tmp/test-mlm \\\n",
        "    --tags \"$language_code,$lm_adapter_training_model_name_or_path\" \\\n",
        "    --logging_steps 99"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A79V3tzHXvKL"
      },
      "outputs": [],
      "source": [
        "!ls -alh /tmp/test-mlm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the Language Adapter to finetune on NER"
      ],
      "metadata": {
        "id": "OPakOvU9emID"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13mHdAYnenlk"
      },
      "outputs": [],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0,1 python3 train_ner_adapter.py --data_dir data/hau/ \\\n",
        "--model_type roberta \\\n",
        "--model_name_or_path $ADAPTER_MODEL \\\n",
        "--output_dir $OUTPUT_DIR \\\n",
        "--max_seq_length  $MAX_LENGTH \\\n",
        "--num_train_epochs $NUM_EPOCHS \\\n",
        "--per_gpu_train_batch_size $BATCH_SIZE \\\n",
        "--save_steps $SAVE_STEPS --learning_rate 5e-4 \\\n",
        "--seed $SEED \\\n",
        "--tags \"$language_code,$lm_adapter_training_model_name_or_path\" \\\n",
        "--path_to_adapter /tmp/test-mlm \\\n",
        "--overwrite_output_dir \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--do_predict"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Backup results to your Google Drive"
      ],
      "metadata": {
        "id": "BKtvKQR0i32Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount your Google Drive"
      ],
      "metadata": {
        "id": "TXwrA24ajJ80"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QRJgs7xVp3a"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backup the language adapter\n",
        "\n",
        "See below for how to backup your language adapter. In this example we use the name of the wandb training run. \n"
      ],
      "metadata": {
        "id": "UpHs4HS0i78o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%env backup_adapter_folder_name dancing-fish-5"
      ],
      "metadata": {
        "id": "k1hAmgrli-pw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AWL5oXUYUTA"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /gdrive/MyDrive/masakhane/double-bind/dancing-fish-5/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5alLUwX5VySY"
      },
      "outputs": [],
      "source": [
        "!cp -rv /tmp/test-mlm/* /gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WF66sMr6YDzc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}